<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <title>AR Racing Track</title>
    <!-- Load A-Frame -->
    <script src="https://aframe.io/releases/1.3.0/aframe.min.js"></script>
    <!-- Load the computer vision library (cv.js) -->
    <script src="js/cv.js"></script>
    <!-- Load the custom 4x4 dictionary file (browser-adapted version) -->
    <script src="js/aruco_4x4_1000.js"></script>
    <!-- Load the main js-aruco2 library -->
    <script src="js/aruco.js"></script>

    <style>
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      html,
      body {
        width: 100%;
        height: 100%;
        overflow: hidden;
        display: flex;
        align-items: center;
        justify-content: center;
        flex-direction: column;
        background: #000;
      }
      h1,
      p {
        color: white;
        position: absolute;
        width: 100%;
        text-align: center;
        z-index: 10;
      }
      h1 {
        top: 10px;
        font-size: 1.5em;
      }
      p {
        top: 50px;
        font-size: 1em;
      }
      /* For debugging, we show the video and canvas.
         You may later hide these elements by setting display:none. */
      #video,
      #canvas {
        position: fixed;
        top: 0;
        left: 0;
        width: 100vw;
        height: 100vh;
        object-fit: cover;
        z-index: 0; 
        border: none;
      }
      

      /* D-Pad and stop button styles from your original project */
      .d-pad {
        position: absolute;
        bottom: 100px;
        left: 50%;
        transform: translateX(-50%);
        display: none; /* currently hidden; show if needed */
        flex-direction: column;
        align-items: center;
        gap: 10px;
      }
      .d-row {
        display: flex;
        gap: 10px;
      }
      .d-button {
        width: 60px;
        height: 60px;
        background-color: rgba(255, 255, 255, 0.6);
        border: 2px solid white;
        border-radius: 10px;
        font-size: 20px;
        font-weight: bold;
        color: black;
        text-align: center;
        line-height: 60px;
        cursor: pointer;
      }
      .d-button:active {
        background-color: rgba(255, 255, 255, 0.9);
      }
      #stopButton {
        position: absolute;
        bottom: 20px;
        left: 50%;
        transform: translateX(-50%);
        display: none;
        background-color: rgba(255, 255, 255, 0.6);
        color: white;
        border: none;
        border-radius: 10px;
        padding: 15px 30px;
        font-size: 18px;
        font-weight: bold;
        cursor: pointer;
      }
      #stopButton:active {
        background-color: rgba(255, 255, 255, 0.9);
      }
    </style>

    <!-- Define our custom A-Frame component -->
    <script>
      // Log to ensure AR and the dictionary are loaded
      console.log("AR object:", AR);
      AFRAME.registerComponent("aruco-detector", {
        init: function () {
          // Get references to the debug video and canvas.
          this.video = document.getElementById("video");
          this.canvas = document.getElementById("canvas");
          this.context = this.canvas.getContext("2d");

          // Create an instance of the AR.Detector (which now uses our custom dictionary)
          try {
            this.detector = new AR.Detector();
            console.log("AR.Detector created:", this.detector);
          } catch (e) {
            console.error("Error creating AR.Detector:", e);
          }

          // Get a reference to the track entity (which contains the racing track model)
          this.trackEl = document.getElementById("track");

          // Request camera access from the environment-facing camera.
          navigator.mediaDevices
            .getUserMedia({ video: { facingMode: "environment" } })
            .then((stream) => {
              this.video.srcObject = stream;
              this.video.play();
              console.log("Camera stream started");
            })
            .catch((err) => {
              console.error("Error accessing camera:", err);
            });
        },
        tick: function () {
          if (this.video.readyState === this.video.HAVE_ENOUGH_DATA) {
            // Set canvas dimensions to match the video
            this.canvas.width = this.video.videoWidth;
            this.canvas.height = this.video.videoHeight;
            // Draw the current video frame on the canvas
            this.context.drawImage(
              this.video,
              0,
              0,
              this.canvas.width,
              this.canvas.height
            );
            // Retrieve the image data from the canvas
            var imageData = this.context.getImageData(
              0,
              0,
              this.canvas.width,
              this.canvas.height
            );
            // Run marker detection (using our custom dictionary)
            var markers = this.detector.detect(imageData);
            console.log("Markers detected:", markers.length);

            if (markers.length > 0) {
              // Instead of using one marker, average the center positions of all detected markers.
              var sumX = 0,
                sumY = 0;
              for (var m = 0; m < markers.length; m++) {
                var corners = markers[m].corners;
                var markerCx = 0,
                  markerCy = 0;
                for (var i = 0; i < corners.length; i++) {
                  markerCx += corners[i].x;
                  markerCy += corners[i].y;
                }
                markerCx /= corners.length;
                markerCy /= corners.length;
                sumX += markerCx;
                sumY += markerCy;
              }
              var avgX = sumX / markers.length;
              var avgY = sumY / markers.length;
              console.log("Average marker center (2D):", avgX, avgY);

              // Show the track entity.
              this.trackEl.setAttribute("visible", "true");

              // Map the average 2D marker position to 3D coordinates.
              // (This mapping is simplified; adjust the multipliers and z-distance for your scene.)
              var x = (avgX / this.canvas.width - 0.5) * 2; // normalized to [-1,1]
              var y = (avgY / this.canvas.height - 0.5) * -2; // inverted y-axis
              var z = -2; // fixed distance from the camera—adjust as needed
              console.log("Mapped 3D position:", x, y, z);

              // Update the 3D position of the track entity.
              this.trackEl.object3D.position.set(x, y, z);
            } else {
              // Hide the track if no markers are detected.
              this.trackEl.setAttribute("visible", "false");
            }
          }
        },
      });
    </script>
  </head>

  <body>
    <h1>AR Racing Track</h1>
    <p>Point your camera at the 4 ArUco markers!</p>
    <!-- Debug video and canvas elements -->
    <video id="video" autoplay playsinline></video>
    <canvas id="canvas"></canvas>
    <!-- A-Frame scene that uses our custom aruco-detector component -->
    <a-scene embedded aruco-detector>
      <!-- The track entity now contains your racing track glTF model -->
      <a-entity id="track" visible="false">
        <a-gltf-model
          src="./models/updatetrack.gltf"
          scale="0.5 0.5 0.5"
          position="0 0 0"
          rotation="90 0 0"
        >
        </a-gltf-model>
      </a-entity>
      <a-entity camera></a-entity>
    </a-scene>
    <!-- (Optional) D-Pad and Stop Button overlays (currently hidden) -->
    <div class="d-pad" id="dPad">
      <div class="d-button">⬆</div>
      <div class="d-row">
        <div class="d-button">⬅</div>
        <div class="d-button">⬇</div>
        <div class="d-button">➡</div>
      </div>
    </div>
    <button id="stopButton">Stop</button>
  </body>
</html>
